#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Dec  7 11:02:35 2020

This script provides an example of how to perform the algorithm comparison and weighted/unweighted
metric calculations using 'custom' (externally executed) algorithms.

Custom algorithms must provide model output, model RMSD, propagated input data uncertainty,
combined (model and input) uncertainty and exist as fields in the matchup database.

This script first generates four synthesised test data sets for 'custom' algorithms, then
performs an example comparison between these test data and the Amazon plume region algorithms.

If the test data has already been generated and added to the matchup database, set 'addTestData'
to False to skip this step.

@author: tom holding
"""

from os import path;
from netCDF4 import Dataset;
import numpy as np;

import osoda_global_settings;
import osoda_algorithm_comparison;


addTestData = True; #When true, four test data sets are synthesised and added to the matchup database.
settings = osoda_global_settings.get_default_settings();


#Create test data, if not already created.
#Naming convention: sdX = standard deviation of X, bY = bias of Y
if addTestData == True:
    def synthesise_data(means, sdNoise, bias):
        return np.random.normal(means+bias, sdNoise);

    matchupTemplate = settings["matchupDatasetTemplate"];
    for year in range(1990, 2020):
        try:
            matchupNC = Dataset(matchupTemplate.safe_substitute(YYYY=year), 'r+');
            print("Synthesising test data sets for year:", year);
            
            dic = matchupNC.variables["DIC_mean"][:];
            at = matchupNC.variables["AT_mean"][:];
            
            #Create two synthesised DIC algorithm output data sets. These are modelled as normal distributions centred on the matchup DIC with SDs and biases of (5, 0) and (10, 3) respectively.
            var = matchupNC.createVariable("test_model_output1_DIC_sd5_b0", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Synthesised data to test algorithm comparison metrics on. Generated by sampling a normal distribution with mean of DIC_mean, SD of 5 and bias of 0";
            var[:] = synthesise_data(dic, 5.0, 0.0);
            var = matchupNC.createVariable("test_model_output1_DIC_sd5_b0_rmsd", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "RMSD of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 5.0);
            var = matchupNC.createVariable("test_model_output1_DIC_sd5_b0_input_uncertainty", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Input uncertainty of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 2.5);
            var = matchupNC.createVariable("test_model_output1_DIC_sd5_b0_combined_uncertainty", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Combined uncertainty of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 7.5);
            
            var = matchupNC.createVariable("test_model_output2_DIC_sd10_b3", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Synthesised data to test algorithm comparison metrics on. Generated by sampling a normal distribution with mean of DIC_mean, SD of 10 and bias of 3";
            var[:] = synthesise_data(dic, 10.0, 3.0);
            var = matchupNC.createVariable("test_model_output2_DIC_sd10_b3_rmsd", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "RMSD of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 10.0);
            var = matchupNC.createVariable("test_model_output2_DIC_sd10_b3_input_uncertainty", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Input uncertainty of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 2.5);
            var = matchupNC.createVariable("test_model_output2_DIC_sd10_b3_combined_uncertainty", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Combined uncertainty of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 12.5);
            
            
            #Create two synthesised AT algorithm output data sets. These are modelled as normal distributions centred on the matchup AT with SDs and biases of (5, 0) and (10, 3) respectively.
            var = matchupNC.createVariable("test_model_output3_AT_sd5_b0", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Synthesised data to test algorithm comparison metrics on. Generated by sampling a normal distribution with mean of AT_mean, SD of 5 and bias of 0";
            var[:] = synthesise_data(at, 5.0, 0.0);
            var = matchupNC.createVariable("test_model_output3_AT_sd5_b0_rmsd", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "RMSD of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 5.0);
            var = matchupNC.createVariable("test_model_output3_AT_sd5_b0_input_uncertainty", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Input uncertainty of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 2.5);
            var = matchupNC.createVariable("test_model_output3_AT_sd5_b0_combined_uncertainty", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Combined uncertainty of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 7.5);
            
            var = matchupNC.createVariable("test_model_output4_AT_sd10_b3", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Synthesised data to test algorithm comparison metrics on. Generated by sampling a normal distribution with mean of AT_mean, SD of 10 and bias of 3";
            var[:] = synthesise_data(at, 10.0, 3.0);
            var = matchupNC.createVariable("test_model_output4_AT_sd10_b3_rmsd", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "RMSD of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 10.0);
            var = matchupNC.createVariable("test_model_output4_AT_sd10_b3_input_uncertainty", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Input uncertainty of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 2.5);
            var = matchupNC.createVariable("test_model_output4_AT_sd10_b3_combined_uncertainty", float, ("index",));
            var.units = "umol kg-1";
            var.long_name = "Combined uncertainty of synthesised data to test algorithm comparison metrics on.";
            var[:] = np.full(dic.shape, 12.5);
    
            
        except FileNotFoundError:
            print("Skipping year {0} because no matchup database file was found.".format(year));
        except RuntimeError as e:
            if e.args[0] == "NetCDF: String match to name in use":
                raise RuntimeError("NetCDF: String match to name in use. This is likely because the test data sets are already in the matchup database. If this is the case, set 'addTestData' to False and rerun.");
            else:
                raise e;



#Define the algorithm information needed to calculate metrics from custom algorithms (name, output var, netCDF variable names for the model output, rmsd and combined uncertainty in the matchup database)
#These match the synthesised test data sets created above, but when using real data they should match the netCDF variable names for the model output, model RMSD, propagated input uncertainty, and combined uncertainty
algoInfo1 = {"name": "test_model_output1_DIC_sd5_b0", #Human readable name, this can be set to anything and is only used as a label
             "outputVar": "DIC", #DIC or AT
             "matchupVariableName": "test_model_output1_DIC_sd5_b0", #netCDF variable name of the model output (algorithm prediction)
             "matchupRMSDName": "test_model_output1_DIC_sd5_b0_rmsd", #netCDF variable name of the RMSD of the (original) algorithm fit
             "inputUncertaintyName": "test_model_output1_DIC_sd5_b0_input_uncertainty", #propagated input data uncertainty
             "matchupCombinedUncertaintyName": "test_model_output1_DIC_sd5_b0_combined_uncertainty", #netCDF variable name of the propagated uncertainty combining input data uncertainty with algorithm fit uncertainty
             
             };
algoInfo2 = {"name": "test_model_output2_DIC_sd10_b3", #Human readable name, this can be set to anything and is only used as a label
             "outputVar": "DIC", #DIC or AT
             "matchupVariableName": "test_model_output2_DIC_sd10_b3", #netCDF variable name of the model output (algorithm prediction)
             "matchupRMSDName": "test_model_output2_DIC_sd10_b3_rmsd", #netCDF variable name of the RMSD of the (original) algorithm fit
             "inputUncertaintyName": "test_model_output2_DIC_sd10_b3_input_uncertainty", #propagated input data uncertainty
             "matchupCombinedUncertaintyName": "test_model_output2_DIC_sd10_b3_combined_uncertainty", #netCDF variable name of the propagated uncertainty combining input data uncertainty with algorithm fit uncertainty
             };
algoInfo3 = {"name": "test_model_output1_AT_sd5_b0", #Human readable name, this can be set to anything and is only used as a label
             "outputVar": "AT", #DIC or AT
             "matchupVariableName": "test_model_output3_AT_sd5_b0", #netCDF variable name of the model output (algorithm prediction)
             "matchupRMSDName": "test_model_output3_AT_sd5_b0_rmsd", #netCDF variable name of the RMSD of the (original) algorithm fit
             "inputUncertaintyName": "test_model_output3_AT_sd5_b0_input_uncertainty", #propagated input data uncertainty
             "matchupCombinedUncertaintyName": "test_model_output3_AT_sd5_b0_combined_uncertainty", #netCDF variable name of the propagated uncertainty combining input data uncertainty with algorithm fit uncertainty
             
             };
algoInfo4 = {"name": "test_model_output2_AT_sd10_b3", #Human readable name, this can be set to anything and is only used as a label
             "outputVar": "AT", #DIC or AT
             "matchupVariableName": "test_model_output4_AT_sd10_b3", #netCDF variable name of the model output (algorithm prediction)
             "matchupRMSDName": "test_model_output4_AT_sd10_b3_rmsd", #netCDF variable name of the RMSD of the (original) algorithm fit
             "inputUncertaintyName": "test_model_output4_AT_sd10_b3_input_uncertainty", #propagated input data uncertainty
             "matchupCombinedUncertaintyName": "test_model_output4_AT_sd10_b3_combined_uncertainty", #netCDF variable name of the propagated uncertainty combining input data uncertainty with algorithm fit uncertainty
             };


#Combine all the custom algorithm information into a single list
#'custom' algorithms do not have python implementations and are algorithms we want to calculate metrics for directly from their model output data + uncertainty
#Note that DIC and AT algorithms can be mixed. The function will sort them accordingly and only compare algorithms that estimate the same parameter.
customAlgorithmInfo = [];
customAlgorithmInfo.append(algoInfo1);
customAlgorithmInfo.append(algoInfo2);
customAlgorithmInfo.append(algoInfo3);
customAlgorithmInfo.append(algoInfo4);


#These are the 'implemented' algorithms you want to compare alongside the custom algorithms. Implemented algorithms have python implementations and therefore
#model outputs, uncertainties etc. are calculated using input data from the matchup database.
#Note that DIC and AT algorithms can be mixed. The function will sort them accordingly and only compare algorithms that estimate the same parameter.
algorithmsToCompare = settings["algorithmRegionMapping"]["oceansoda_amazon_plume"]; #Here we're just selecting all the algorithms for the amazon region

osoda_algorithm_comparison.custom_algorithm_metrics(settings,
                                                    algorithmsToCompare=algorithmsToCompare, #These are non-custom algorithms. These have python implementations and therefore the model output, rmsd and combined uncertainty can be calculated from input data provided in the matchup database
                                                    customAlgorithmInfo=customAlgorithmInfo, #Custom algorithms are those which do not have python implementations in the "algorithms" directory. I.e. the model output, rmsd and combined uncertainty are supplied externally via the matchup database
                                                    region="oceansoda_amazon_plume", #Which region to use. This selects the spatial mask used to subset the matchup database
                                                    sstDatasetName="SST-ESACCI-OSTIA", #which SST data set was used to produce the custom algorithm output? Name must match the 'datasetName' field from one of the DatasetInfo entries for settings["datasetInfoMap"] (in osoda_global_settings.py) for SST. These are currently "SST-ESACCI-OSTIA", "SST-CORA" or "SST-OSTIA".
                                                    sssDatasetName="SSS-ESACCI-SMOS",  #which SSS data set was used to produce the custom algorithm output? Name must match the 'datasetName' field from one of the DatasetInfo entries for settings["datasetInfoMap"] (in osoda_global_settings.py) for SSS. These are currently "SSS-ESACCI-SMOS", "SSS-CORA", "SSS-RSS-SMAP" or "SSS-ISAS".
                                                    outputRoot=path.join("output/example_test_custom_algo_metrics/"), #Where to write output files to
                                                    diagnosticPlots=True, #this will create model vs in situ matchup database plots (visual aid to goodness of fit, useful for debugging)
                                                    );

